The FBK system was built upon a standard phrase-based system using the Moses toolkit \cite{koehn07:moses}, and exploited the huge amount of parallel English-French and monolingual French training data provided by the organizers.
%
It featured a statistical log-linear model including a phrase-based translation model  and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word and phrase penalties.
%
Tuning of the system was performed on dev2010 by optimizing BLEU using Minimum Error Rate Training \cite{och03:mer}.

It is worth noticing that all available development data sets, namely dev2010 and test2010-2012, were added to the in-domain training data to build the system actually employed for the 2014 evaluation campaign.

In order to adapt the system on TED specific domain and genre, and to reduce the size of the system, data selection was carried out on all parallel English-French corpora, using all WIT$^3$ training corpus as in-domain data.

Data selection was performed by means of XenC toolkit \cite{Rousseau:2013:pbml} exploiting the bilingual cross-entropy difference \cite{axelrod-he-gao:2011:EMNLP} mode, separately for each available training corpora, but the in-domain WIT$^3$ data.
%
Different amount of texts were selected from each corpora ranging from 2\% and 30\%, and then concatenating for building one parallel corpus containing 57M English and 63M French running words in total.

Two TMs and two RMs were trained on the parallel in-domain and selected data, respectively, and combined using the back-off technique (for both TM and RM), taking WIT$^3$ as primary component, for a total of 168M phrase pairs.
Back-off table combination is similar to the fill-up technique  \cite{Bisazza:11b}, but does not add any provenance binary features.
The French side of in-domain and selected data were also employed to estimate a mixture language model \cite{Federico:98}. Moreover, a second huge French LM was estimated on all permitted monolingual French data consisting of about 1.4G running words. 
%
Both LMs have order 5 and were smoothed by means of the interpolated Improved Kneser-Ney method \cite{Chen:98a}; they include 57M and 661M 5-grams, respectively.

